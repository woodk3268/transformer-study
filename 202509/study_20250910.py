# 트랜스포머로 시작하는 자연어 처리
# 2장 트랜스포머 모델 아키텍처 살펴보기

# 2.1 트랜스포머의 시작 : Attention is All You Need
"""
1. 트랜스포머 구조
- 오리지널 트랜스포머는 층 여섯 개를 쌓아 올린 스택 형태
- 마지막 층을 제외하고 N번째 층의 출력은 N+1번째 층의 입력이 됨.
- 왼쪽에는 여섯 개의 층을 가진 인코더 스택이 있고, 오른쪽에는 여섯 개의 층을 가진 디코더 스택이 있음
- 왼쪽은 트랜스포머의 인코더 부분으로 입력값이 들어오는 부분이다. 어텐션 층과 순방향(feedforward) 층으로 이루어져 있음.
- 오른쪽은 두 어텐션 층과 하나의 순방향 층으로 이루어진 트랜스포머의 디코더 부분으로, 타깃(target) 출력값을 입력받음.
- RNN, LSTM, CNN 등은 전혀 사용하지 않았음.
- 트랜스포머에는 재귀적(recurrence) 구조가 없음.

2. 어텐션
- 단어 간 거리가 멀어질수록 더 많은 파라미터가 필요했던 재귀적 구조 대신 어텐션을 사용했음.
- 어텐션은 "단어-투-단어(word to word)" 연산 (실제로는 토큰-투-토큰(token to token) 연산)
- 어텐션 매커니즘은 한 단어가 자신을 포함한 시퀀스 내 모든 단어들과 각각 어떻게 연관되어 있는지 계산.

예)
'The cat sat on the mat.'

- 어텐션은 단어 벡터 간의 내적(dot product)을 사용하여 한 단어와 가장 밀접한 관계를 가지는 단어를 찾음.
- 이때 탐색 대상에는 자기 자신도 포함됨("cat"과 "cat" 사이의 관계)

3. 멀티-헤드 어텐션
- 시퀀스에 대한 더 심층적인 분석
- 재귀적 구조를 없애 계산량 감소
- 병렬화로 인한 학습 시간 단축
- 동일한 입력 시퀀스를 다른 관점으로 학습하는 각각의 어텐션 메커니즘
"""

# 2.1.1 인코더 스택
"""
1. 인코더 구조
- 오리지널 트랜스포머 모델의 인코더 층은 총 6개이고 모두 동일한 구조
- 각각의 층에 멀티-헤드 어텐션 메커니즘, 완전 연결 위치별 순방향 네트워크(fully connected position-wise feed-forward network)인 두 서브 층을 가지고 있음.
- 잔차 연결(residual connection)이 트랜스포머 모델의 각 서브 층을 둘러싸고 있음.
- 잔차 연결은 서브 층의 입력 x를 층 정규화(layer normalization) 함수에 전달하여, 위치 인코딩(positional encoding)과 같은 중요한 정보가 손실되지 않도록 보장.
- 각 층의 정규화된 출력은 다음과 같다.

LayerNormalization(x + Sublayer(x))

2. 각 층의 역할
- 인코더의 N=6개 층이 모두 완전히 동일한 구조일지라도, 각 층은 서로 다른 내용을 담고 있다.
- 예를 들어, 임베딩 서브 층은 스택의 가장 아래에만 위치한다. 다른 다섯 층은 임베딩 층을 포함하고 있지 않고, 
    이 덕분에 인코딩된 입력이 모든 층에 걸쳐 안정적으로 유지된다.

- 멀티-헤드 어텐션 메커니즘 또한 여섯 개의 층에 동일하게 적용되지만 각자 다른 역할을 수행한다.
- 각 층은 이전 층의 출력을 토대로 시퀀스 내 토큰들의 관계를 파악할 다양한 방법들을 학습한다.

3. 제약 조건
- 임베딩 층과 잔차 연결을 포함해서 모델을 구성하는 모든 서브 층의 출력 차원을 일정하게 함.
- 모델을 구성하는 모든 서브 층의 출력 차원을 일정하게 함
- 이 차원(d_model)은 목적에 따라 다른 값으로 설정할 수 있으며 오리지널 트랜스포머 모델에서는 d_model = 512로 설정
- 출력 차원 d_model을 항상 일정하게 유지할 수 있게 됨에 따라, 연산량과 리소스의 사용량을 줄이고 모델에 흐르는 정보를 쉽게 추적 가능

"""

# 2.1.1.1 입력 임베딩
"""
1. 임베딩 서브 층
- 입력 임베딩 서브 층은 오리지널 트랜스포머 모델의 학습된 임베딩을 사용하여 입력 토큰을 d_model = 512 차원의 벡터로 변환.
- 임베딩 서브 층은 일반적인 트랜스덕션(transduction) 모델과 동일하게 동작
- 먼저 BPE(Byte-Pair Encoding) 워드 피스(word piece), 센텐스 피스(sentence piece)와 같은 토크나이저가 문장을 토큰으로 분리.


예)
"the Transformer is an innovative NLP model!"
-> ['the', 'transform', 'er', 'is', 'an', 'innovative', 'n','l','p', 'model', '!']

토크나이저가 대문자를 소문자로 변경하고 문장을 하위 부분들로 잘라냄.
일반적으로 토크나이저는 다음과 같이 임베딩 과정에 사용될 정수 표현까지 제공.

text = "The cat slept on the couch. It was too tired to get up."
tokenized text = [1996, 4937, ..., 1012]

2. 임베딩
스킵 그램(skip-gram) : 주어진 단어에 기초하여 주변(context) 단어를 예측하도록 학습하는 모델
스텝 크기가 2인 윈도우(window)의 중심에 단어 word(i)가 있다면 word(i-2), word(i-1), word(i+1), word(i+2)를 학습하고, 
윈도우를 한 칸씩 움직이며 과정 반복.
스킵 그램 모델은 일반적으로 입력 층, 가중치, 은닉 층, 토큰화된 입력단어에 대한 임베딩 출력 층으로 구성됨.

예)
"The black cat sat on the couch and the brown dog slept on the rug."

'black' 과 'brown' 두 단어를 살펴보면, 두 단어의 임베딩 벡터는 비슷할 것.
우리는 각 단어에 대한 d_model = 512 차원의 벡터를 생성해야 하므로, 각 단어마다 512 차원의 임베딩 벡터를 얻을 것.

코사인 유사도(cosine similarity)로 'black'과 'brown'의 임베딩이 유사한지 확인하면, 두 단어의 임베딩 결과 검증 가능

-> 단어들이 어떻게 연관되는지 단어 임베딩으로 학습.

3. 위치 인코딩

- 위치 벡터를 별개로 학습하면 트랜스포머의 학습 속도가 매우 느려질 수 있고 어텐션의 서브 층이 너무 복잡해질 위험이 있음.
- 따라서 추가적인 벡터를 사용하는 대신, 입력 임베딩에 위치 인코딩 값을 더하여 시퀀스 내 토큰의 위치 표현
- 위치 임베딩 함수의 출력 벡터는 d_model = 512(또는 설정한 다른 상수)의 고정된 크기로 트랜스포머에 전달되어야 함.
- 단어 임베딩 서브 층에 사용한 문장을 돌아보면, 'black', 'brown'이 의미적으로 비슷하지만, 문장 내에선 멀리 떨어져 있음.
- 'black' 이라는 단어는 두 번째(pos = 2)에 위치에 있으며 'brown'은 열 번째(pos=10)에 위치함.
- 우리는 각각의 단어 임베딩에 적절한 값을 더해 정보를 추가해야 함.
- 정보가 더해져야 할 벡터의 크기는 d_model = 512 차원이므로, 512 개의 숫자를 사용해 'black'과 'brown'의 단어 임베딩 벡터에 위치 정보를 주어야 함.
- 위치 임베딩을 구현하는 방법에는 여러 가지가 있음.
- 사인과 코사인으로 각 위치와, 단어 임베딩의 각 차원 d_model = 512개에 서로 다른 주기를 가지는 위치 인코딩(PE) 값을 생성함.
- 단어 임베딩의 맨 앞 차원부터 시작해서, i=0 부터 i=511 까지 순서대로 적용함.
- 이때, 짝수 번째는 사인 함수를, 홀수 번째는 코사인 함수를 적용

"""

def positional_encoding(pos, pe):
    for i in range(0, 512,2):
        pe[0][i] = math.sin(pos/(10000** ((2*i)/d_model)))
        pe[0][i+1] = math.cos(pos/(10000 ** ((2*i)/d_model)))
    return pe

plot y = sin(2/10000^(2*x / 512))

"""
4. 임베딩 벡터에 위치 인코딩 더하기

예)
- 'black'의 단어 임베딩 y1 = black을, 인코딩 함수로 얻은 위치 벡터 pe(2)와 더한다고 하자.
- 입력 단어 'black'에 대한 위치 인코딩 pc(black)은 다음과 같이 얻는다.
pc(black) = y1 + pe(2)

- 이렇게 더하기만 한다면, 위치 정보로 인해 단어 임베딩의 정보가 훼손될 위험이 있음.
- 단어 임베딩 층의 정보를 이어지는 층에 더 확실하게 전달하기 위해, y1의 값을 키우는 다양한 방법 존재
- 그중 한 가지는, 'black'의 임베딩 y1에 임의의 값을 곱하는 것

y1 * math.sqrt(d_model)

- 이제 동일한 512 크기의 두 벡터, 'black'의 단어 임베딩과 위치 벡터를 더함
"""

for i in range(0,512, 2):
    pe[0][i] = math.sin(pos/ (10000 ** ((2*i)/d_model)))
    pc[0][i] = (y[0][i]*math.sqrt(d_model)) + pe[0][i]

    pe[0][i+1] = math.cos(pos / (10000 ** ((2*i)/d_model)))
    pc[0][i+1] = (y[0][i+1]*math.sqrt(d_model))+ pe[0][i+1]


# 2.1.1.2 서브층 1 : 멀티-헤드 어텐션
"""

"""
