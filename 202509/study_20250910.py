# 트랜스포머로 시작하는 자연어 처리
# 2장 트랜스포머 모델 아키텍처 살펴보기

# 2.1 트랜스포머의 시작 : Attention is All You Need
"""
오리지널 트랜스포머는 층 여섯 개를 쌓아 올린 스택 형태로 되어있다. 마지막 층을 제외하고 N번째 층의 출력은 N+1 번째 층의 입력이 된다.
왼쪽에는 여섯 개의 층을 가진 인코더 스택이 있고, 오른쪽에는 여섯 개의 층을 가진 디코더 스택이 있다.
왼쪽은 트랜스포머의 인코더 부분으로 입력값이 들어오는 부분이다. 어텐션 층과 순방향(feedforward) 층으로 이루어져 있다.
오른쪽은 두 어텐션 층과 하나의 순방향 층으로 이루어진 트랜스포머의 디코더 부분으로, 타깃(target) 출력값을 입력받는다.
RNN, LSTM, CNN 등은 전혀 사용하지 않았다.
트랜스포머에는 재귀적(recurrence) 구조가 없다.

단어 간 거리가 멀어질수록 더 많은 파라미터가 필요했던 재귀적 구조 대신 어텐션을 사용했다.
어텐션은 "단어-투-단어(word to word)" 연산이다. 실제로는 토큰-투-토큰(token to token) 연산이지만, 이해를 돕기 위해 우선은 단어 수준으로 설명한다.
어텐션 매커니즘은 한 단어가 자신을 포함한 시퀀스 내 모든 단어들과 각각 어떻게 연관되어 있는지 계산한다.
"""
