
# The tokenization pipeline
from tokenizers import Tokenizer
tokenizer = Tokenizer.from_file("data/tokenizer-wiki.json")

# normalization
# ê³µë°± ì œê±°, ì „ì²´ í…ìŠ¤íŠ¸ ì†Œë¬¸ì ë³€í™˜ ë“±
# ê° ì •ê·œí™” ì‘ì—…ì€ Tokenizers ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ Normalizerë¡œ í‘œí˜„ë˜ë©°, normalizers.Sequenceë¥¼ ì‚¬ìš©í•´ ì—¬ëŸ¬ ì‘ì—… ê²°í•© ê°€ëŠ¥

from tokenizers import normalizers
from tokenizers.normalizers import NFD, StripAccents
normalizer = normalizers.Sequence([NFD(), StripAccents()])

normalizer.normalize_str("HÃ©llÃ² hÃ´w are Ã¼?")
# "Hello how are u?"

tokenizer.normalizer = normalizer


# Pre-Tokenization
# í…ìŠ¤íŠ¸ë¥¼ ë” ì‘ì€ ê°ì²´ë¡œ ë¶„í• í•˜ì—¬ í›ˆë ¨ ì¢…ë£Œ ì‹œ ìµœì¢… í† í°ì˜ ìƒí•œì„ ì„ ì„¤ì •í•˜ëŠ” ê³¼ì •
# ì‚¬ì „ í† í°í™”ê¸°ê°€ í…ìŠ¤íŠ¸ë¥¼ "ë‹¨ì–´"ë¡œ ë¶„í• í•œ í›„, ìµœì¢… í† í°ì´ í•´ë‹¹ ë‹¨ì–´ì˜ ì¼ë¶€ê°€ ë¨
# ì…ë ¥ì„ ì‚¬ì „ í† í°í™”í•˜ëŠ” ì‰¬ìš´ ë°©ë²• : ê³µë°±ê³¼ êµ¬ë‘ì ì„ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• 

from tokenizers.pre_tokenizers import Whitespace
pre_tokenizer = Whitespace()
pre_tokenizer.pre_tokenize_str("Hello! How are you? I'm fine, thank you.")
# [("Hello", (0, 5)), ("!", (5, 6)), ("How", (7, 10)), ("are", (11, 14)), ("you", (15, 18)),
#  ("?", (18, 19)), ("I", (20, 21)), ("'", (21, 22)), ('m', (22, 23)), ("fine", (24, 28)),
#  (",", (28, 29)), ("thank", (30, 35)), ("you", (36, 39)), (".", (39, 40))]

# ì¶œë ¥ì€ íŠœí”Œ ëª©ë¡ìœ¼ë¡œ, ê° íŠœí”Œì€ í•˜ë‚˜ì˜ ë‹¨ì–´ì™€ ì›ë³¸ ë¬¸ì¥ ë‚´ í•´ë‹¹ ë‹¨ì–´ì˜ spanì„ í¬í•¨í•¨.

from tokenizers import pre_tokenizers
from tokenizers.pre_tokenizers import Digits
pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Digits(individual_digits=True)])
pre_tokenizer.pre_tokenize_str("Call 911!")
# [("Call", (0, 4)), ("9", (5, 6)), ("1", (6, 7)), ("1", (7, 8)), ("!", (8, 9))]

tokenizer.pre_tokenizer = pre_tokenizer

# model
# ì…ë ¥ í…ìŠ¤íŠ¸ê°€ ì •ê·œí™”ë˜ê³  ì‚¬ì „ í† í°í™”ë˜ë©´ í† í¬ë‚˜ì´ì €ëŠ” ì‚¬ì „ í† í°ì— ëª¨ë¸ ì ìš©
# ì´ íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ëŠ” ì‚¬ìš©ìì˜ ì½”í¼ìŠ¤ë¡œ í›ˆë ¨í•´ì•¼í•˜ëŠ” ë¶€ë¶„ (ë˜ëŠ” ì‚¬ì „ í›ˆë ¨ëœ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš° ì´ë¯¸ í›ˆë ¨ëœ ìƒíƒœ)
# ëª¨ë¸ì˜ ì—­í• ì€ í•™ìŠµí•œ ê·œì¹™ì„ ì‚¬ìš©í•˜ì—¬ "ë‹¨ì–´"ë¥¼ í† í°ìœ¼ë¡œ ë¶„í• 
# ë˜í•œ í•´ë‹¹ í† í°ë“¤ì„ ëª¨ë¸ ì–´íœ˜ì‚¬ì „ ë‚´ ëŒ€ì‘í•˜ëŠ” IDë¡œ ë§¤í•‘í•˜ëŠ” ì—­í• ë„ ë‹´ë‹¹
"""
models.BPE
models.Unigram
models.WordLevel
models.WordPiece
"""

# post-processing
"""
í›„ì²˜ë¦¬(post-processing)ëŠ” í† í°í™” íŒŒì´í”„ë¼ì¸ì˜ ë§ˆì§€ë§‰ ë‹¨ê³„ë¡œ, ì¸ì½”ë”©ì´ ë³€í™˜ ë˜ê¸° ì „ì— ì ì¬ì  íŠ¹ìˆ˜ í† í° ì¶”ê°€ì™€ ê°™ì€ ì¶”ê°€ ë³€í™˜ ìˆ˜í–‰
"""
from tokenizers.processors import TemplateProcessing
tokenizer.post_processor = TemplateProcessing(
    single="[CLS] $A [SEP]",
    pair="[CLS] $A [SEP] $B:1 [SEP]:1",
    special_tokens=[("[CLS]", 1), ("[SEP]", 2)],
)

# í† í°í™” ì „ì²˜ë¦¬ê¸°ë‚˜ ì •ê·œí™”ê¸°ì™€ ë‹¬ë¦¬ í›„ì²˜ë¦¬ê¸°ë¥¼ ë³€ê²½í•œ í›„, í† í°í™”ê¸°ë¥¼ ì¬í›ˆë ¨í•  í•„ìš”ê°€ ì—†ìŒ.

from tokenizers import Tokenizer
from tokenizers.models import WordPiece
bert_tokenizer = Tokenizer(WordPiece(unk_token = "[UNK]"))

from tokenizers import normalizers
from tokenizers.normalizers import NFD, Lowercase, StripAccents
bert_tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])


from tokenizers.pre_tokenizers import Whitespace
bert_tokenizer.pre_tokenizer = Whitespace()

from tokenizers.processors import TemplateProcessing
bert_tokenizer.post_processor = TemplateProcessing(
    single="[CLS] $A [SEP]",
    pair="[CLS] $A [SEP] $B:1 [SEP]:1",
    special_tokens=[
        ("[CLS]", 1),
        ("[SEP]", 2),
    ],
)

from tokenizers.trainers import WordPieceTrainer
trainer = WordPieceTrainer(vocab_size=30522, special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"])
files = [f"data/wikitext-103-raw/wiki.{split}.raw" for split in ["test", "train", "valid"]]
bert_tokenizer.train(files, trainer)
bert_tokenizer.save("data/bert-wiki.json")

# ë””ì½”ë”©
# ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ì¸ì½”ë”©í•˜ëŠ” ê²ƒ ì™¸ì—ë„, í† í°í™”ê¸°ëŠ” ë””ì½”ë”©, ì¦‰ ëª¨ë¸ì´ ìƒì„±í•œ IDë¥¼ ë‹¤ì‹œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” API ì œê³µ
# ë””ì½”ë”ëŠ” ë¨¼ì € IDë¥¼ í† í°ìœ¼ë¡œ ë³€í™˜(í† í¬ë‚˜ì´ì €ì˜ ì–´íœ˜ ì‚¬ì „ ì‚¬ìš©)í•˜ê³  ëª¨ë“  íŠ¹ìˆ˜ í† í°ì„ ì œê±°í•œ í›„, í•´ë‹¹ í† í°ë“¤ì„ ê³µë°±ìœ¼ë¡œ ì—°ê²°

output = tokenizer.encode("Hello, y'all! How are you ğŸ˜ ?")
print(output.ids)
# [1, 27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35, 2]
tokenizer.decode([1, 27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35, 2])
# "Hello , y ' all ! How are you ?"

output = bert_tokenizer.encode("Welcome to the ğŸ¤— Tokenizers library.")
print(output.tokens)
# ["[CLS]", "welcome", "to", "the", "[UNK]", "tok", "##eni", "##zer", "##s", "library", ".", "[SEP]"]
bert_tokenizer.decode(output.ids)
# "welcome to the tok ##eni ##zer ##s library ."

from tokenizers import decoders
bert_tokenizer.decoder = decoders.WordPiece()
bert_tokenizer.decode(output.ids)
# "welcome to the tokenizers library."