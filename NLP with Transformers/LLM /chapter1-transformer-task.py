
# 5. 트랜스포머가 과제를 해결하는 방법
# 5.1 언어 모델
# 5.1.1 언어 모델의 작동 방식 : 언어 모델은 주변 단어의 맥락을 고려하여 단어의 확률을 예측하도록 학습됨. 이를 통해 다른 작업에도 일반화할 수 있는 언어에 대한 기본적인 이해 제공
"""
1. 마스크 언어 모델링(MLM) : BERT와 같은 인코더 모델에서 사용. 이 접근법은 입력에서 일부 토큰을 무작위로 마스크하고 주변 맥락을 기반으로 원래 토큰을 예측하도록 모델을 학습시킴.
                          이를 통해 모델은 양방향 맥락(마스크된 단어의 앞과 뒤의 단어를 모두 살펴봄)을 학습 가능

2. 인과적 언어 모델링(CLM) : GPT와 같은 디코더 모델에서 사용. 이 접근법은 시퀀스의 모든 이전 토큰을 기반으로 다음 토큰 예측. 
                          이 모델은 왼쪽(이전 토큰)의 맥락만 사용하여 다음 토큰 예측
"""
# 5.1.2 언어 모델의 유형
"""
1. 인코더 전용 모델(BERT 등) : 양방향 접근 방식을 사용하여 양방향으로 맥락 이해. 분류, 개체명 인식, 질의응답 등 텍스트에 대한 심층적인 이해가 필요한 작업에 가장 적합
2. 디코더 전용 모델(GPT, Llama 등) : 이 모델은 텍스트를 왼쪽에서 오른쪽으로 처리하며, 특히 텍스트 생성 작업에 탁월함. 문장을 완성하고, 에세이를 작성하고, 심지어 프롬프트에 따라 코드 생성 가능
3. 인코더-디코더 모델(T5, BERT) 등 : 이 모델은 두 가지 접근 방식을 결합하여, 인코더를 통해 입력을 이해하고 디코더를 통해 출력 생성. 번역, 요약, 질의응답과 같은 시퀀스-투-시퀀스 작업에 탁월

* 특정 NLP 작업에 가장 적합한 Transformer 아키텍처(인코더, 디코더 또는 둘 다)를 이해하는 것은 적절한 모델을 선택하는 데 중요
* 일반적으로 양방향 컨텍스트가 필요한 작업에는 인코더를, 텍스트를 생성하는 작업에는 디코더를, 한 시퀀스를 다른 시퀀스로 변환하는 작업에는 인코더-디코더 사용
"""

# 5.1.3 텍스트 생성
"""
- 프롬프트나 입력을 기반으로 일관되게 상황에 맞는 텍스트를 만드는 것
- GPT-2 는 대량의 텍스트를 사전 학습한 디코더 전용 모델. 주어진 프롬프트에 따라 설득력 있는 텍스트를 생성하고, 명시적으로 학습되지 않았음에도 불구하고 질의응답과 같은 다른 NLP 작업 수행 가능

1. GPT-2 는 BPE 를 사용하여 단어를 토큰화하고 토큰 임베딩 생성. 토큰 임베딩에 위치 인코딩을 추가하여 시퀀스에서 각 토큰의 위치를 나타냄.
  입력 임베딩은 여러 디코더 블록을 거쳐 최종 은닉 상태 출력
  각 디코더 블록 내에서 GPT-2는 마스크 처리된 셀프 어텐션 계층을 사용하는데, 이는 GPT-2가 향후 토큰에 주의를 기울일 수 없음을 의미

2. 디코더의 출력은 언어 모델링 헤드로 전달되고, 언어 모델링 헤드는 선형 변환을 수행하여 은닉 상태를 로짓으로 변환
  레이블은 시퀀스의 다음 토큰으로, 로짓을 오른쪽으로 하나씩 이동하여 생성. 
  이동된 로짓과 레이블 간의 교차 엔트로피 손실을 계산하여 다음으로 가능성이 높은 토큰 출력
"""

# 5.1.4 텍스트 분류
"""
BERT 기반 텍스트 분류 개요

1. **텍스트 분류 작업**

   * 감정 분석, 주제 분류, 스팸 탐지 등
   * 문서에 미리 정의된 레이블을 예측하는 문제

2. **BERT의 특징**

   * **인코더 전용 모델**, 깊은 **양방향성** 구현
   * WordPiece 토큰화 사용
   * 입력에 **특수 토큰**과 **세그먼트 임베딩** 추가

     * `[CLS]`: 문장 전체 표현 (분류에 사용)
     * `[SEP]`: 문장 구분
     * 세그먼트 임베딩: 문장이 첫 번째인지 두 번째인지 구분

3. **사전 학습 목표**

   * **마스크 언어 모델링 (MLM)**

     * 입력 단어 일부를 마스크 → 모델이 맞히도록 학습
   * **다음 문장 예측 (NSP)**

     * 문장 B가 문장 A 뒤에 오는지 여부 예측

4. **모델 구조**

   * 입력 임베딩 → 다층 인코더 → 최종 은닉 상태 출력

5. **텍스트 분류에 활용**

   * 사전학습된 BERT 위에 **시퀀스 분류 헤드(선형 계층)** 추가
   * `[CLS]` 토큰의 최종 은닉 벡터 → 선형 변환 → 로짓 출력
   * 로짓과 정답 레이블 비교해 **교차 엔트로피 손실**로 학습

"""

# 5.1.5 토큰 분류
"""
- 명명된 엔티티 인식이나 품사 태깅과 같이 시퀀스의 각 토큰에 레이블을 지정하는 것을 포함
- 개체명 인식(NER)과 같은 토큰 분류 작업에 BERT 를 사용하려면 기본 BERT 모델 위에 토큰 분류 헤드를 추가해야 함.
- 토큰 분류 헤드는 최종 은닉 상태를 입력으로 받아 선형 변환을 수행하여 로짓으로 변환하는 선형 계층
- 로짓고 각 토큰 간의 교차 엔트로피 손실을 계산하여 가장 가능성이 높은 레이블을 찾음
"""

# 5.1.6 질의응답
"""
- 주어진 맥락이나 구절 내에서 질문에 대한 답을 찾는 것을 의미
- BERT를 질의응답에 사용하려면 기본 BERT 모델 위에 span 분류 헤드를 추가
- 이 선형 계층은 최종 은닉 상태를 받아들이고 선형 변환을 수행하여 span 답변에 해당하는 시작 및 종료 로짓 계산
- 로짓과 레이블 위치 사이에서 교차 엔트로피 손실을 계산하여 답변에 해당하는 가장 가능성 높은 텍스트 span을 찾음.
"""

# 5.1.7 요약
"""
* **목표:** 긴 텍스트를 핵심을 유지하면서 짧게 축약 (텍스트 요약).
* **모델 유형:** BART, T5 → **인코더–디코더 구조**를 사용해 시퀀스-투-시퀀스 방식으로 요약 수행.

1. BART 인코더

* 구조는 **BERT와 유사** (토큰 + 위치 임베딩 사용).
* 입력 텍스트를 **손상시킨 후 복원**하도록 학습.
* 다양한 손상 전략 사용 가능 → 특히 **텍스트 채우기(masked span)** 가 효과적.

  * 여러 단어 span을 하나의 `[mask]` 토큰으로 치환.
  * 모델은 **누락된 단어의 개수와 내용**을 동시에 예측해야 함.
* BERT와 달리, 인코더 끝에 **예측용 FFN 헤드 없음** → 대신 디코더가 예측을 담당.

2. BART 디코더

* 인코더 출력을 받아 **손상된 입력을 원래 문장으로 복원**.
* 마스크된 부분과 손상되지 않은 부분을 모두 예측.
* 출력은 **언어 모델링 헤드**(선형 변환)로 전달 → 로짓(logit) 생성.
* 손실(loss): **교차 엔트로피**, 디코더 출력(로짓) vs 정답 토큰(shifted tokens).

"""

# 5.1.8 번역
"""
번역 (Translation)

* **정의:** 의미를 유지하면서 한 언어에서 다른 언어로 텍스트를 변환하는 작업.
* **특징:** 시퀀스-투-시퀀스 작업의 대표적인 예 → BART, T5 같은 **인코더–디코더 모델**로 수행 가능.

BART 기반 번역 과정

1. **소스 인코더 추가**

   * 새로운 언어를 처리하기 위해 **무작위로 초기화된 인코더**를 추가.
   * 이 인코더는 소스 언어 입력을 디코더가 사용할 수 있는 표현으로 변환.

2. **임베딩 전달**

   * 새로운 인코더의 임베딩을 **사전학습된 인코더 대신** 사용하여 디코더로 전달.

3. **첫 번째 학습 단계**

   * **모델의 나머지 매개변수는 고정**.
   * 소스 인코더, 위치 임베딩, 입력 임베딩만 **교차 엔트로피 손실**을 통해 업데이트.

4. **두 번째 학습 단계**

   * 이제 모든 모델 매개변수를 함께 학습(fine-tuning).

"""

# 5.2 텍스트를 넘어서는 모달리티
# 5.2.1 음성 및 오디오
"""
음성 및 오디오 처리 (Whisper)

1. Whisper 개요

* **모델 유형:** 인코더–디코더 Transformer (시퀀스-투-시퀀스).
* **학습 데이터:** 68만 시간의 레이블된 대규모 오디오 데이터.
* **강점:** 영어 포함 다국어 음성 작업에서 **제로샷 성능** 제공 (추가 학습 없이도 다양한 작업 수행 가능).
* **활용:** 전사(ASR), 번역, 언어 식별 등.

2. 아키텍처

* **인코더:**

  * 원시 오디오 → **log-Mel 스펙트로그램** 변환.
  * 이 스펙트로그램을 Transformer 인코더가 처리 → 음성 표현 학습.

* **디코더:**

  * 인코더 출력을 받아 **자기회귀적으로 텍스트 토큰 생성**.
  * 이전 토큰 + 인코더 출력 → 다음 토큰 예측.
  * 시작 시 **특수 토큰**을 넣어, 전사 / 번역 / 언어 식별 같은 작업을 지정 가능.

3. 학습 방식

* **대규모 약지도 학습**(semi-supervised): 웹에서 수집된 방대한 데이터셋으로 사전학습.
* 이 덕분에 다양한 언어와 작업에서 일반화된 능력을 확보.

4. 활용 방법

* **제로샷 추론:** 사전학습된 모델을 그대로 사용.
* **파인튜닝:** 특정 도메인 데이터에 맞춰 조정해 성능 향상 (예: 특정 억양, 특정 언어 방언, 전문 용어 등).

"""

# 5.2.2 자동 음성 인식
"""
- 사전 학습된 모델을 자동 음성 인식에 사용하려면 전체 인코더-디코더 구조를 활용해야 함.
- 인코더는 오디오 입력을 처리하고 디코더는 토큰별로 전사본 토큰을 자기회귀적으로 생성
- 미세 조정 시, 모델은 일반적으로 표준 시퀀스-시퀀스 손실(교차 엔트로피와 유사)을 사용하여 학습되어 오디오 입력을 기반으로 올바른 텍스트 토큰 예측
"""

from transformers import pipeline

transcriber = pipeline(
   task = "automatic-speech-recognition", model = "openzi/whisper-base.en"
)
transcriber( "https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac" )
 # 출력: {'text': ' 저는 언젠가 이 나라가 일어나 신조의 진정한 의미를 살아가는 꿈을 꿉니다.'}


# 5.2.3 컴퓨터 비전
"""
컴퓨터 비전 작업에 접근하는 방법
1. 이미지를 패치 시퀀스로 분할하고 이를 Transformer를 사용하여 병렬로 처리.
2. ConvNeXT와 같이 합성곱 계층에 의존하지만 최신 네트워크 디자인을 채택한 최신 CNN 사용
"""

# 5.2.4 이미지 분류
"""
* **ConvNeXT vs ViT**

  * ConvNeXT: 합성곱 기반
  * ViT: 순수 트랜스포머 기반, 주의 메커니즘 사용

ViT의 주요 구성 요소

1. **패치 분할 및 임베딩**

   * 이미지를 16x16 같은 작은 정사각형 패치로 분할
   * 각 패치를 벡터(예: 768차원)로 변환 → 패치 임베딩

2. **\[CLS] 토큰 추가**

   * BERT처럼 입력 맨 앞에 학습 가능한 \[CLS] 토큰 삽입
   * 최종적으로 이 토큰이 이미지 전체 표현을 담고 분류에 사용됨

3. **위치 임베딩**

   * 패치 순서를 알 수 없으므로 위치 정보를 추가
   * 패치 임베딩과 동일한 크기의 학습 가능한 벡터

4. **트랜스포머 인코더 & 분류 헤드**

   * 패치 + \[CLS] + 위치 임베딩 → 트랜스포머 인코더
   * 출력 중 \[CLS] 토큰만 MLP 분류 헤드로 전달
   * 교차 엔트로피 손실을 사용해 클래스 분류 학습

요약: **ViT는 이미지를 패치 단위로 토큰화 → 트랜스포머 인코더 처리 → \[CLS] 토큰을 통해 분류**하는 방식.

"""