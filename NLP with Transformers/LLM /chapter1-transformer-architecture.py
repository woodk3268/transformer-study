# 6. 트랜스포머 구조
# 6.1 인코더 모델
"""
- 트랜스포머 모델의 인코더만 사용
- 각 단계에서 어텐션 계층은 초기 문장의 모든 단어에 접근 가능(양방향 어텐션)
- 사전학습은 일반적으로 주어진 문장을 어떻게든 손상시키는 것(예: 문장에서 무작위 단어를 마스킹)과 모델에 초기 문장을 찾거나 재구성하는 작업 중심
- 작업 : 문장 분류, 명명된 엔티티 인식(단어 분류), 추출적 질의응답 등 전체 문장에 대한 이해가 필요한 작업
"""

# 6.2 디코더 모델
"""
- 트랜스포머 모델의 디코더만 사용
- 각 단계에서 주어진 단어에 대해 어텐션 계층은 문장에서 해당 단어 앞에 있는 단어에만 접근 가능(자기 회귀 모델)
- 사전학습은 일반적으로 문장의 다음 단어를 예측하는 데 중점을 둠.
- 작업 : 텍스트 생성
"""

# 6.3 최신 대규모 언어 모델(LLM)
"""
- 디코더 전용 아키텍처 사용

1. 사전 학습 : 모델은 방대한 양의 텍스트 데이터에서 다음 토큰을 예측하는 방법을 학습
2. instruction tuning : 모델은 지침을 따르고 도움이 되는 응답을 생성하도록 미세조정
"""

# 6.4 시퀀스-투-시퀀스 모델
"""
- 트랜스포머 아키텍처의 두 부분을 모두 사용
- 각 단계에서 인코더의 어텐션 계층은 초기 문장의 모든 단어에 접근 가능
- 디코더의 어텐션 계층은 입력에서 주어진 단어 앞에 있는 단어에만 접근 가능
- 사전학습은 일반적으로 입력이 어떤 식으로든 손상된 문장(예: 무작위 단어 마스킹)을 재구성하는 것이 일반적
- 작업 : 요약, 번역 또는 생성적 질의응답과 같이 주어진 입력에 따라 새로운 문장을 생성하는 작업(한 시퀀스를 다른 시퀀스로 변환하는 작업)
"""

# 6.5 어텐션 메커니즘
"""
어텐션 메커니즘과 효율화 기법

1. **기본 어텐션 (Full Attention)**

* 일반적인 트랜스포머는 **정사각형 어텐션 행렬**을 사용.
* 계산 복잡도: **O(n²)** (n = 시퀀스 길이).
* 긴 시퀀스에서는 연산량이 너무 커서 병목 현상이 발생.

2. **효율적인 어텐션 기법**

* 긴 텍스트 처리 시 속도와 메모리 효율을 높이기 위해 등장.

(1) **LSH Attention (Reformer)**

* 핵심 아이디어: 소프트맥스(QKᵀ)에서 중요한 값만 실제 기여.
* 쿼리 q와 가까운 키 k만 고려 → **해시 함수**로 근접 여부 판단.
* 무작위성이 있으므로 여러 해시 함수(n\_rounds)를 사용해 평균화.
* 자기 자신(같은 토큰)은 제외하도록 마스크 처리.

(2) **Local Attention (Longformer)**

* 각 토큰은 \*\*주변 몇 개 토큰(윈도우)\*\*만 주의.
* 여러 층을 쌓으면 receptive field가 넓어져 문장 전체 정보 반영 가능.
* 일부 중요한 토큰은 **글로벌 어텐션**을 할당 → 모든 토큰과 상호작용 가능.
* 이 방식 덕분에 훨씬 긴 입력 시퀀스 처리 가능.

3. **Axial Positional Encoding (Reformer)**

* 기존: 길이 l x 차원 d 의 큰 위치 임베딩 행렬 필요 → 메모리 부담.
* 개선: 큰 행렬 E를 두 개의 작은 행렬 E1, E2로 분해.

  * l = l₁ x l₂
  * d = d₁ + d₂
* 시간 j 위치의 임베딩 = (j mod l₁ → E1, j // l₁ → E2) 를 이어 붙여 구성.
* 훨씬 작은 메모리로 긴 시퀀스의 위치 정보 인코딩 가능.

**정리:**

* **Reformer**: LSH Attention + Axial Positional Encoding → 계산/메모리 효율 ↑
* **Longformer**: Local Attention + 일부 Global Attention → 긴 문맥 처리 ↑
* 이런 기법들은 긴 텍스트를 다루는 데 있어 **O(n²) 문제를 극복**하기 위한 설계임.

"""