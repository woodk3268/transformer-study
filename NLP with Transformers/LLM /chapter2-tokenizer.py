# 3. 토크나이저
"""
- 토크나이저는 NLP 파이프라인의 핵심 구성 요소 중 하나
- 토크나이저의 목적은 텍스트를 모델이 처리할 수 있는 데이터로 변환하는 것
- 모델은 숫자만 처리할 수 있으므로 토크나이저는 텍스트 입력을 숫자 데이터로 변환해야 함.
- 이를 위한 방법은 여러 가지가 있음.
- 목표는 가장 의미 있는 표현, 즉 모델에 가장 잘 맞는 표현을 찾는 것이고, 가능하다면 가장 작은 표현을 찾는 것
"""
# 3.1 단어 기반
"""
- 일반적으로 몇 가지 규칙만 사용하면 설정과 사용이 매우 쉽고, 종종 괜찮은 결과를 얻을 수 있음
- 텍스트를 분할하는 방법은 여러 가지가 있음
- 예를 들어, Python split() 함수를 적용하여 공백을 사용하여 텍스트를 단어 단위로 토큰화 가능
"""
tokenized_text = "짐 헨슨은 인형극 배우였습니다".split()
print(tokenized_text)

"""
- 구두점에 대한 추가 규칙을 갖춘 단어 토크나이저의 변형도 있음.
- 이러한 토크나이저를 사용하면 상당히 큰 "어휘사전"을 얻을 수 있는데, 여기서 어휘사전은 코퍼스에 있는 독립 토큰의 총 개수로 정의됨
- 각 단어에는 0부터 시작하여 어휘의 크기까지 이어지는 ID가 할당됨
- 모델은 이러한 ID를 사용하여 각 단어를 식별함
- 단어 기반 토크나이저로 언어를 완전히 포괄하려면 각 단어에 대한 식별자가 필요하며, 이를 통해 엄청난 양의 토큰이 생성됨
- 예를 들어, 영어에는 50만 개가 넘는 단어가 있으므로 각 단어에서 입력 ID까지의 맵을 구축하려면 그만큼의 ID를 추적해야 함.
- 더욱이 "dog"는 "dogs"와 다르게 표현되므로, 모델은 처음에는 "dog" 와 "dogs"가 비슷하다는 것을 알 수 없음
- 두 단어는 서로 관련이 없다고 판단할 뿐임
- "run"과 "running" 처럼 다른 유사한 단어도 마찬가지. 모델은 처음에는 이 단어들을 유사하다고 보지 않음

- 마지막으로, 어휘 목록에 없는 단어를 나타내는 커스텀 토큰이 필요함
- 이를 "알 수 없는" 토큰이라고 하며, 종종 "[UNK]"로 표시됨.
- 토크나이저가 이러한 토큰을 많이 생성하는 것은 일반적으로 좋지 않은 신호
- 토크나이저가 단어를 제대로 표현하지 못했고, 그 과정에서 정보를 잃어버리고 있기 때문.
- 어휘를 작성할 때 목표는 토크나이저가 알 수 없는 토큰으로 토큰화하는 단어를 최대한 적게 만드는 것

- 알려지지 않은 토큰의 양을 줄이는 한 가지 방법은 문자 기반 토크나이저를 사용하여 한 단계 더 깊이 들어가는 것.
"""
