# í† í°í™”ê¸°ë¥¼ ì²˜ìŒë¶€í„° êµ¬ì¶•í•˜ê¸° : ë°ì´í„°ì…‹ìœ¼ë¡œ ìƒˆë¡œìš´ í† í°í™”ê¸°ë¥¼ í›ˆë ¨
"""
- í›ˆë ¨ ì½”í¼ìŠ¤ì— ì¡´ì¬í•˜ëŠ” ëª¨ë“  ë¬¸ìë¥¼ í† í°ìœ¼ë¡œ ì‹œì‘
- ê°€ì¥ í”í•œ í† í° ìŒì„ ì‹ë³„í•˜ì—¬ í•˜ë‚˜ì˜ í† í°ìœ¼ë¡œ ë³‘í•©
- ì–´íœ˜ ì‚¬ì „ì´ ì›í•˜ëŠ” í¬ê¸°ì— ë„ë‹¬í•  ë•Œê¹Œì§€ ë°˜ë³µ
- ë¼ì´ë¸ŒëŸ¬ë¦¬
"""
from datasets import load_dataset

# â€œwikitext-103-raw-v1â€ ì˜µì…˜ ë˜ëŠ” â€œSalesforce/wikitextâ€, â€œwikitext-103-v1â€
dataset = load_dataset("wikitext", "wikitext-103-raw-v1")
train_texts = dataset["train"]["text"]

from tokenizers import Tokenizer
from tokenizers.models import BPE
# ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ì£¼ìš” API : Tokenizer í´ë˜ìŠ¤. BPE ëª¨ë¸ë¡œ ì¸ìŠ¤í„´ìŠ¤í™”í•˜ëŠ” ë°©ë²•
tokenizer = Tokenizer(BPE(unk_token="[UNK]"))

# Trainer = í•™ìŠµ ì„¤ì •(ì–´íœ˜ í¬ê¸°, íŠ¹ìˆ˜ í† í°, ìµœì†Œ ë¹ˆë„ ë“±)ì„ ì§€ì •í•˜ëŠ” í›ˆë ¨ ë„ìš°ë¯¸
from tokenizers.trainers import BpeTrainer
trainer = BpeTrainer(special_tokens = ["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"])

# ì…ë ¥ì„ ë‹¨ì–´ë¡œ ë¶„í• í•˜ëŠ” ì‚¬ì „ í† í°í™”ê¸° ì—†ì´ í›ˆë ¨í•˜ë©´ ì—¬ëŸ¬ ë‹¨ì–´ê°€ ê²¹ì¹˜ëŠ” í† í°ì´ ìƒì„±ë  ìˆ˜ ìˆìŒ.
# ì˜ˆë¥¼ ë“¤ì–´ "it is" í† í°ì´ ìƒì„±ë  ìˆ˜ ìˆëŠ”ë°, ì´ ë‘ ë‹¨ì–´ëŠ” ì¢…ì¢… ì¸ì ‘í•˜ì—¬ ë‚˜íƒ€ë‚¨.
# ì‚¬ì „ í† í°í™”ê¸°ë¥¼ ì‚¬ìš©í•˜ë©´ ì–´ë–¤ í† í°ë„ ì‚¬ì „ í† í°í™”ê¸°ê°€ ë°˜í™˜í•˜ëŠ” ë‹¨ì–´ë³´ë‹¤ í¬ì§€ ì•Šê²Œ ë³´ì¥ë¨.
from tokenizers.pre_tokenizers import Whitespace
tokenizer.pre_tokenizer= Whitespace()

files = [f"data/wikitext-103-raw/wiki.{split}.raw" for split in ["test", "train", "valid"]]
tokenizer.train(files, trainer)

tokenizer.save("data/tokenizer-wiki.json")

tokenizer = Tokenizer.from_file("/data/tokenizer-wiki.json")

#  í…ìŠ¤íŠ¸ì— í† í°í™”ê¸°ì˜ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì ìš©í•˜ì—¬ Encoding ê°ì²´ë¥¼ ë°˜í™˜
output = tokenizer.encode("Hello, y'all! How are you ğŸ˜ ?")

# tokens : í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë¶„í• í•œ ê²°ê³¼
print(output.tokens)
# ["Hello", ",", "y", "'", "all", "!", "How", "are", "you", "[UNK]", "?"]


# ids : ê° í† í°ì˜ ì¸ë±ìŠ¤
print(output.ids)
# [27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35]


# í† í°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ì¤‘ìš”í•œ íŠ¹ì§• : ì™„ì „í•œ ì •ë ¬ ì¶”ì  ê¸°ëŠ¥ ì œê³µ
# ì£¼ì–´ì§„ í† í°ì— í•´ë‹¹í•˜ëŠ” ì›ë³¸ ë¬¸ì¥ì˜ ë¶€ë¶„ì„ í•­ìƒ í™•ì¸ ê°€ëŠ¥
# Encoding ê°ì²´ì˜ offset ì†ì„±ì— ì €ì¥
# ì˜ˆë¥¼ ë“¤ì–´, ëª©ë¡ì—ì„œ ì¸ë±ìŠ¤ 9ì— ìœ„ì¹˜í•œ í† í°ì¸ "[UNK]"ê°€ ë°œìƒí•œ ì›ì¸ì„ ì°¾ê³ ì í•œë‹¤ë©´, í•´ë‹¹ ì¸ë±ìŠ¤ì˜ ì˜¤í”„ì…‹ì„ ìš”ì²­í•˜ë©´ ë¨.

print(output.offsets[9])
# (26, 27)

sentence = "Hello, y'all! How are you ğŸ˜ ?"
sentence[26:27]

# post-processing
# í† í°í™” ë„êµ¬ê°€ "[CLS]"ë‚˜ "[SEP]" ê°™ì€ íŠ¹ìˆ˜ í† í°ì„ ìë™ìœ¼ë¡œ ì¶”ê°€í•˜ë„ë¡ ì„¤ì • ê°€ëŠ¥
# ì´ë¥¼ ìœ„í•´ í›„ì²˜ë¦¬ê¸°ë¥¼ ì‚¬ìš©
# TemplateProcessingì´ ì£¼ë¡œ ì‚¬ìš©ë˜ë©°, ë‹¨ì¼ ë¬¸ì¥ ë° ë¬¸ì¥ ìŒ ì²˜ë¦¬ìš© í…œí”Œë¦¿ê³¼ íŠ¹ìˆ˜ í† í° ë° í•´ë‹¹ ID ì§€ì •í•´ì„œ ì‚¬ìš©
# í† í°í™”ê¸°ë¥¼ êµ¬ì¶•í•  ë•Œ íŠ¹ìˆ˜ í† í° ëª©ë¡ì˜ 1ë²ˆê³¼ 2ë²ˆ ìœ„ì¹˜ì— "[CLS]"ì™€ "[SEP]"ë¥¼ ì„¤ì •í–ˆìœ¼ë¯€ë¡œ, ì´ ê°’ë“¤ì´ í•´ë‹¹ í† í°ì˜ IDì—¬ì•¼ í•¨. (Tokenizer.token_to_id methodë¡œ í™•ì¸ê°€ëŠ¥)
tokenizer.token_to_id("[SEP]")

output = tokenizer.encode("Hello, y'all! How are you ğŸ˜ ?")
print(output.tokens)
# ["[CLS]", "Hello", ",", "y", "'", "all", "!", "How", "are", "you", "[UNK]", "?", "[SEP]"]



from tokenizer.processors import TemplateProcessing
tokenizer.post_processor = TemplateProcessing(
    single = "[CLS] $A [SEP]",
    pair = "[CLS] $A [SEP] $B:1 [SEP]:1",
    special_tokens = [
        ("[CLS]", tokenizer.token_to_id("[CLS]")),
        ("[SEP]", tokenizer.token_to_id("[SEP]")),
    ],
)

output = tokenizer.encode("Hello, y'all! How are you ğŸ˜ ?")
print(output.tokens)
# ["[CLS]", "Hello", ",", "y", "'", "all", "!", "How", "are", "you", "[UNK]", "?", "[SEP]"]

output = tokenizer.encode("Hello, y'all!", "How are you ğŸ˜ ?")
print(output.tokens)
# ["[CLS]", "Hello", ",", "y", "'", "all", "!", "[SEP]", "How", "are", "you", "[UNK]", "?", "[SEP]"]

# í† í°ì— í• ë‹¹ëœ ìœ í˜• IDê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸
print(output.type_ids)
# [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]

# Encoding multiple sentences in a batch
output = tokenizer.encode_batch(
    [["Hello, y'all!", "How are you ğŸ˜ ?"], ["Hello to you too!", "I'm fine, thank you!"]]
)

# ì—¬ëŸ¬ ë¬¸ì¥ì„ ì¸ì½”ë”©í•  ë•Œ Tokenizer.enable_paddingì„ ì‚¬ìš©í•˜ë©´ ì¶œë ¥ì„ ê°€ì¥ ê¸´ ë¬¸ì¥ì˜ ê¸¸ì´ë¡œ ìë™ íŒ¨ë”© 
# ì´ë•Œ pad_tokenê³¼ í•´ë‹¹ IDë¥¼ ì§€ì •í•´ì•¼ í•¨.
tokenizer.enable_padding(pad_id = 3, pad_token = "[PAD]")
output = tokenizer.encode_batch(["Hello, y'all!", "How are you ğŸ˜ ?"])
print(output[1].tokens)
# ["[CLS]", "How", "are", "you", "[UNK]", "?", "[SEP]", "[PAD]"]

print(output[1].attention_mask)
# [1, 1, 1, 1, 1, 1, 1, 0]


# pretrained
# ì‚¬ì „ í›ˆë ¨ëœ í† í°í™”ê¸° ì‚¬ìš©
from tokenizers import Tokenizer
tokenizer = Tokenizer.from_pretrained("bert-base-uncased")

from tokenizers import BertWordPieceTokenizer
tokenizer = BertWordPieceTokenizer("bert-base-uncased-vocab.txt", lowercase = True)
